{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vgg_gpu_v2_with_wb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aljeshishe/notebooks/blob/master/vgg_gpu_v2_with_wb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYN4BJRj04iP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf \n",
        "from tensorflow import keras\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import sys \n",
        "\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten \n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from time import time "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuyI62a3Yn7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset():\n",
        "    #load dataset\n",
        "    (trainX, trainY),(testX, testY) = cifar10.load_data()\n",
        "    #one hot encode the target \n",
        "    trainY = keras.utils.to_categorical(trainY)\n",
        "    testY = keras.utils.to_categorical(testY)\n",
        "    return trainX, trainY, testX, testY\n",
        "\n",
        "def validation_split(testX, testY, valid_X, valid_Y, v_split):\n",
        "    \n",
        "    index_of_validation = int(v_split * len(testX))\n",
        "    valid_X.extend(testX[-index_of_validation:])\n",
        "    valid_Y.extend(testY[-index_of_validation:])\n",
        "    testX = testX[:-index_of_validation]\n",
        "    testY = testY[:-index_of_validation]\n",
        "    return testX, testY, np.asarray(valid_X), np.asarray(valid_Y)\n",
        "\n",
        "def normalize(train,test,valid):\n",
        "    # convert from integers to float \n",
        "    train_norm = train.astype('float32')\n",
        "    test_norm = test.astype('float32')\n",
        "    valid_norm = valid.astype('float32')\n",
        "    #normalize to range 0-1\n",
        "    train_norm = train_norm / 255.0\n",
        "    test_norm = test_norm / 255.0\n",
        "    valid_norm = valid_norm / 255.0\n",
        "    return train_norm, test_norm,valid_norm \n",
        "\n",
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(history):\n",
        "    plt.subplots(figsize = (7,7))\n",
        "    # plot loss\n",
        "    plt.subplot(211)\n",
        "    plt.title('Cross Entropy Loss')\n",
        "    plt.plot(history.history['loss'], color='blue', label='train')\n",
        "    plt.plot(history.history['val_loss'], color='orange', label='test')\n",
        "\n",
        "    # plot accuracy\n",
        "    plt.subplot(212)\n",
        "    plt.title('Classification Accuracy')\n",
        "    plt.plot(history.history['accuracy'], color='blue', label='train')\n",
        "    plt.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        "    plt.show()\n",
        "    # save plot to file\n",
        "    filename = sys.argv[0].split('/')[-1]\n",
        "    plt.savefig(filename + '_plot.png')\n",
        "    plt.close()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DykMdnMxPNsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load dataset\n",
        "trainX, trainY, testX, testY = load_dataset()\n",
        "#get validation set \n",
        "valid_X = []\n",
        "valid_Y = []\n",
        "testX, testY, validX, validY = validation_split(testX, testY, valid_X, valid_Y,v_split=0.5)\n",
        "\n",
        "# normalize the data\n",
        "trainX, testX,validX = normalize(trainX, testX,validX)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKmeruBkZna8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "# use gpu/cpu/tpu\n",
        "# see details in https://colab.research.google.com/drive/1cpuwjKTJbMjlvZ7opyrWzMXF_NYnjkiE#scrollTo=y3gk7nSvTUFZ\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "COLAB_TPU_ADDR = os.environ.get('COLAB_TPU_ADDR')\n",
        "if COLAB_TPU_ADDR:\n",
        "  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + COLAB_TPU_ADDR)\n",
        "  tf.config.experimental_connect_to_cluster(resolver)\n",
        "  # This is the TPU initialization code that has to be at the beginning.\n",
        "  tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "  strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "  print('Running on TPU ')  \n",
        "elif len(gpus) > 1:\n",
        "  strategy = tf.distribute.MirroredStrategy([gpu.name for gpu in gpus])\n",
        "  print('Running on multiple GPUs ', [gpu.name for gpu in gpus])\n",
        "elif len(gpus) == 1:\n",
        "  strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
        "  print('Running on single GPU ', gpus[0].name)\n",
        "else:\n",
        "  strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
        "  print('Running on CPU')\n",
        "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiD_iLrWBaGJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install wandb -q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke6le2McoCpE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "config = dict(\n",
        "  batch_size = 64,          # input batch size for training (default: 64)\n",
        "  epochs = 1000,             # number of epochs to train (default: 10)\n",
        "  lr = 0.001,               # learning rate (default: 0.01)\n",
        "  momentum = 0.9,          # SGD momentum (default: 0.5) \n",
        "  seed = 42,               # random seed (default: 42)\n",
        "  log_interval = 10,     # how many batches to wait before logging training status\n",
        "  weight_decay = 0.0005\n",
        ")\n",
        "wandb.init(config=config)\n",
        "config = wandb.config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--8PDw7LAq8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vgg():\n",
        "  input_shape = (trainX.shape[1], trainX.shape[2], 3)\n",
        "  \n",
        "  # Define the model architecture - This is a simplified version of the VGG19 architecture\n",
        "  model = Sequential()\n",
        "  \n",
        "  # Set of Conv2D, Conv2D, MaxPooling2D layers with 32 and 64 filters\n",
        "  model.add(Conv2D(filters = 32, kernel_size = (3, 3), padding = 'same', \n",
        "                    activation ='relu', input_shape = input_shape, kernel_regularizer=regularizers.l2(config.weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.3))\n",
        "\n",
        "  model.add(Conv2D(filters = 64, kernel_size = (3, 3), padding = 'same', \n",
        "                    activation ='relu', input_shape = input_shape, kernel_regularizer=regularizers.l2(config.weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  # Another set of Conv2D, Conv2D, MaxPooling2D layers with 128 filters\n",
        "  model.add(Conv2D(filters = 128, kernel_size = (3, 3), padding = 'same', \n",
        "                    activation ='relu', input_shape = input_shape, kernel_regularizer=regularizers.l2(config.weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Conv2D(filters = 128, kernel_size = (3, 3), padding = 'same', \n",
        "                    activation ='relu', input_shape = input_shape, kernel_regularizer=regularizers.l2(config.weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  # Another set of Conv2D, Conv2D, MaxPooling2D layers with 256 filters\n",
        "  model.add(Conv2D(filters = 256, kernel_size = (3, 3), padding = 'same', \n",
        "                    activation ='relu', input_shape = input_shape, kernel_regularizer=regularizers.l2(config.weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Conv2D(filters = 256, kernel_size = (3, 3), padding = 'same', \n",
        "                    activation ='relu', input_shape = input_shape, kernel_regularizer=regularizers.l2(config.weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  # Another set of Conv2D, Conv2D, MaxPooling2D layers with 512 filters\n",
        "  model.add(Conv2D(filters = 512, kernel_size = (3, 3), padding = 'same', \n",
        "                    activation ='relu', input_shape = input_shape, kernel_regularizer=regularizers.l2(config.weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Conv2D(filters = 512, kernel_size = (3, 3), padding = 'same', \n",
        "                    activation ='relu', input_shape = input_shape, kernel_regularizer=regularizers.l2(config.weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  # Another set of Conv2D, Conv2D, MaxPooling2D layers with 512 filters\n",
        "  model.add(Conv2D(filters = 1024, kernel_size = (3, 3), padding = 'same', \n",
        "                    activation ='relu', input_shape = input_shape, kernel_regularizer=regularizers.l2(config.weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Conv2D(filters = 1024, kernel_size = (3, 3), padding = 'same', \n",
        "                    activation ='relu', input_shape = input_shape, kernel_regularizer=regularizers.l2(config.weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  # Flatten\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512, activation ='relu', kernel_regularizer=regularizers.l2(config.weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.5))\n",
        "  num_classes = 10\n",
        "  model.add(Dense(num_classes, activation = \"softmax\"))\n",
        "  opt = SGD(lr = config.lr, momentum=config.momentum)\n",
        "  model.compile(optimizer=opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYxyiIGoPQ3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define model\n",
        "def baseline():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32,(3,3), activation = 'relu', kernel_initializer = 'he_uniform', padding = 'same', input_shape = (32,32,3)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(32,(3,3), activation = 'relu', kernel_initializer = 'he_uniform', padding = 'same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D((2,2)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv2D(64,(3,3), activation = 'relu', kernel_initializer = 'he_uniform', padding = 'same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(64,(3,3), activation = 'relu', kernel_initializer = 'he_uniform', padding = 'same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D((2,2)))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Conv2D(128,(3,3), activation = 'relu', kernel_initializer = 'he_uniform', padding = 'same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(128,(3,3), activation = 'relu', kernel_initializer = 'he_uniform', padding = 'same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D((2,2)))\n",
        "    model.add(Dropout(0.4))\n",
        "   \n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation = 'relu', kernel_initializer = 'he_uniform'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(10, activation = 'softmax'))\n",
        "    \n",
        "    #compile model \n",
        "    opt = SGD(lr = config.lr, momentum=config.momentum)\n",
        "    model.compile(optimizer=opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwhpW4_UPRlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with strategy.scope():\n",
        "  model = vgg()\n",
        "#epochs = 400\n",
        "#create data generator \n",
        "datagen = ImageDataGenerator(width_shift_range = 0.1, height_shift_range = 0.1, horizontal_flip = True)\n",
        "#iterator \n",
        "train = datagen.flow(trainX, trainY, batch_size=config.batch_size)\n",
        "# fit model\n",
        "steps = int(trainX.shape[0]/ 64)\n",
        "history = model.fit_generator(train, steps_per_epoch=steps, epochs=config.epochs, validation_data=(validX, validY), verbose=1,\n",
        "                              callbacks=[WandbCallback(validation_data=(validX, validY),\n",
        "                                                       save_model=True, verbose=1)])\n",
        "\n",
        "wandb.save(\"mymodel.h5\")\n",
        "model.save(os.path.join(wandb.run.dir, \"mymodel.h5\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpXCa1AtPSv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate model\n",
        "_, acc = model.evaluate(testX, testY, verbose=0)\n",
        "print('> %.3f' % (acc * 100.0))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv-ai0wLPV-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "summarize_diagnostics(history)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfHX5GRi1a7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wandb.save(\"mymodel.h5\")\n",
        "model.save(os.path.join(wandb.run.dir, \"mymodel.h5\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuVx4a1S_a9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}